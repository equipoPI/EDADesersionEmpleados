{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuál es la puntuación de satisfacción laboral esperada para un empleado, considerando el balance trabajo-vida, la reputación de la empresa y las oportunidades de liderazgo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Características: Balance Trabajo-Vida, Reputación de la Empresa y Oportunidadez de Liderazgo\n",
    "* Clasificacion: Satisfaccion de Trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar y dividir datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50214, 3), (12554, 3), (50214, 3), (12554, 3), (50214,), (12554,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "archivo = 'archivo_estandarizado_normalizado.csv'\n",
    "df = pd.read_csv(archivo)\n",
    "\n",
    "# Selección de las características (X) y la variable objetivo (Y)\n",
    "X = df[['Horas Extras+norm', 'Oportunidades de Innovación+norm', 'Nivel de Trabajo+norm']]\n",
    "Y = df['Satisfacción del Trabajo+norm']\n",
    "\n",
    "# Divide el 80% para entrenamiento y 20% para validación/prueba\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Divide el 20% restante en partes iguales para validación y prueba (10% y 10%)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Tamaño de X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}\")\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "archivo = 'archivo_estandarizado_normalizado.csv'\n",
    "df = pd.read_csv(archivo)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Selección de columnas de interés para la predicción de la \"Satisfacción del Trabajo\"\n",
    "target_column = 'Satisfacción del Trabajo'\n",
    "\n",
    "# Variables de interés en versiones estandarizadas\n",
    "feature_columns_estandarizadas = [\n",
    "    'Balance Trabajo-Vida+estand',\n",
    "    'Reputación de la Empresa+estand',\n",
    "    'Oportunidades de Liderazgo+estand'\n",
    "]\n",
    "\n",
    "# Variables de interés en versiones normalizadas\n",
    "feature_columns_normalizadas = [\n",
    "    'Balance Trabajo-Vida+norm',\n",
    "    'Reputación de la Empresa+norm',\n",
    "    'Oportunidades de Liderazgo+norm'\n",
    "]\n",
    "\n",
    "# Definir las variables predictoras y la variable objetivo\n",
    "X_estandarizado = df[feature_columns_estandarizadas]\n",
    "X_normalizado = df[feature_columns_normalizadas]\n",
    "y = df[target_column]\n",
    "\n",
    "# Dividir los datos en entrenamiento (80%) y prueba (20%) para ambas versiones\n",
    "X_train_est, X_test_est, y_train, y_test = train_test_split(X_estandarizado, y, test_size=0.2, random_state=42)\n",
    "X_train_norm, X_test_norm, _, _ = train_test_split(X_normalizado, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verificar las formas de los conjuntos de entrenamiento y prueba\n",
    "X_train_est.shape, X_test_est.shape, X_train_norm.shape, X_test_norm.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento con SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Estandarizado': {'R2': 0.0002959474023764841,\n",
       "  'RMSE': np.float64(0.9684200151778256),\n",
       "  'MAE': np.float64(0.789577699521196)},\n",
       " 'Normalizado': {'R2': 0.0002959474023764841,\n",
       "  'RMSE': np.float64(0.9684200151778256),\n",
       "  'MAE': np.float64(0.7895776995211962)}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar el modelo de Regresión Lineal\n",
    "linear_model_est = LinearRegression()\n",
    "linear_model_norm = LinearRegression()\n",
    "\n",
    "# Entrenar el modelo en los datos estandarizados\n",
    "linear_model_est.fit(X_train_est, y_train)\n",
    "\n",
    "# Entrenar el modelo en los datos normalizados\n",
    "linear_model_norm.fit(X_train_norm, y_train)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "y_pred_est = linear_model_est.predict(X_test_est)\n",
    "y_pred_norm = linear_model_norm.predict(X_test_norm)\n",
    "\n",
    "# Calcular las métricas para el conjunto estandarizado\n",
    "r2_est = r2_score(y_test, y_pred_est)\n",
    "rmse_est = np.sqrt(mean_squared_error(y_test, y_pred_est))\n",
    "mae_est = mean_absolute_error(y_test, y_pred_est)\n",
    "\n",
    "# Calcular las métricas para el conjunto normalizado\n",
    "r2_norm = r2_score(y_test, y_pred_norm)\n",
    "rmse_norm = np.sqrt(mean_squared_error(y_test, y_pred_norm))\n",
    "mae_norm = mean_absolute_error(y_test, y_pred_norm)\n",
    "\n",
    "# Resultados\n",
    "{\n",
    "    'Estandarizado': {'R2': r2_est, 'RMSE': rmse_est, 'MAE': mae_est},\n",
    "    'Normalizado': {'R2': r2_norm, 'RMSE': rmse_norm, 'MAE': mae_norm}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Random Forest Estandarizado': {'R2': 0.0012046791073027086,\n",
       "  'RMSE': np.float64(0.9679797678626253),\n",
       "  'MAE': np.float64(0.78712556059863)},\n",
       " 'Random Forest Normalizado': {'R2': 0.0012046791073027086,\n",
       "  'RMSE': np.float64(0.9679797678626253),\n",
       "  'MAE': np.float64(0.78712556059863)},\n",
       " 'KNN Estandarizado': {'R2': -0.15795277030511534,\n",
       "  'RMSE': np.float64(1.0422537191293832),\n",
       "  'MAE': np.float64(0.8604588179066434)},\n",
       " 'KNN Normalizado': {'R2': -0.2860896305248686,\n",
       "  'RMSE': np.float64(1.0984079025317113),\n",
       "  'MAE': np.float64(0.8894694917954437)}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Inicializar los modelos\n",
    "rf_model_est = RandomForestRegressor(random_state=42)\n",
    "rf_model_norm = RandomForestRegressor(random_state=42)\n",
    "knn_model_est = KNeighborsRegressor()\n",
    "knn_model_norm = KNeighborsRegressor()\n",
    "\n",
    "# Entrenar los modelos en los datos estandarizados\n",
    "rf_model_est.fit(X_train_est, y_train)\n",
    "knn_model_est.fit(X_train_est, y_train)\n",
    "\n",
    "# Entrenar los modelos en los datos normalizados\n",
    "rf_model_norm.fit(X_train_norm, y_train)\n",
    "knn_model_norm.fit(X_train_norm, y_train)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba para Bosques Aleatorios\n",
    "y_pred_rf_est = rf_model_est.predict(X_test_est)\n",
    "y_pred_rf_norm = rf_model_norm.predict(X_test_norm)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba para K-Vecinos Más Cercanos\n",
    "y_pred_knn_est = knn_model_est.predict(X_test_est)\n",
    "y_pred_knn_norm = knn_model_norm.predict(X_test_norm)\n",
    "\n",
    "# Calcular las métricas para el modelo de Bosques Aleatorios en ambos conjuntos\n",
    "r2_rf_est = r2_score(y_test, y_pred_rf_est)\n",
    "rmse_rf_est = np.sqrt(mean_squared_error(y_test, y_pred_rf_est))\n",
    "mae_rf_est = mean_absolute_error(y_test, y_pred_rf_est)\n",
    "\n",
    "r2_rf_norm = r2_score(y_test, y_pred_rf_norm)\n",
    "rmse_rf_norm = np.sqrt(mean_squared_error(y_test, y_pred_rf_norm))\n",
    "mae_rf_norm = mean_absolute_error(y_test, y_pred_rf_norm)\n",
    "\n",
    "# Calcular las métricas para el modelo de K-Vecinos Más Cercanos en ambos conjuntos\n",
    "r2_knn_est = r2_score(y_test, y_pred_knn_est)\n",
    "rmse_knn_est = np.sqrt(mean_squared_error(y_test, y_pred_knn_est))\n",
    "mae_knn_est = mean_absolute_error(y_test, y_pred_knn_est)\n",
    "\n",
    "r2_knn_norm = r2_score(y_test, y_pred_knn_norm)\n",
    "rmse_knn_norm = np.sqrt(mean_squared_error(y_test, y_pred_knn_norm))\n",
    "mae_knn_norm = mean_absolute_error(y_test, y_pred_knn_norm)\n",
    "\n",
    "# Resultados\n",
    "{\n",
    "    'Random Forest Estandarizado': {'R2': r2_rf_est, 'RMSE': rmse_rf_est, 'MAE': mae_rf_est},\n",
    "    'Random Forest Normalizado': {'R2': r2_rf_norm, 'RMSE': rmse_rf_norm, 'MAE': mae_rf_norm},\n",
    "    'KNN Estandarizado': {'R2': r2_knn_est, 'RMSE': rmse_knn_est, 'MAE': mae_knn_est},\n",
    "    'KNN Normalizado': {'R2': r2_knn_norm, 'RMSE': rmse_knn_norm, 'MAE': mae_knn_norm}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento con Bosques Aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores características para Satisfacción de Trabajo+norm y sus puntuaciones:\n",
      "                               Feature         Score\n",
      "1                          Edad+estand  5.462192e+17\n",
      "2      Antigüedad en la Empresa+estand  1.690627e+04\n",
      "7  Meses desde el último evento+estand  2.529194e+03\n",
      "0                          ID Empleado  1.906651e+03\n",
      "3                Rol de Trabajo+estand  1.746140e+03\n",
      "6             Tamaño de Empresa+estand  1.060711e+03\n",
      "4               Ingreso Mensual+estand  9.291019e+02\n",
      "5              Distancia a Casa+estand  7.860848e+02\n",
      "9      Reputación de la Empresa+estand  2.527854e+02\n",
      "8                Trabajo Remoto+estand  1.571239e+02\n",
      "\n",
      "Mejores características para Satisfacción de Trabajo+estand y sus puntuaciones:\n",
      "                             Feature         Score\n",
      "1                          Edad+norm  2.737709e+17\n",
      "2      Antigüedad en la Empresa+norm  1.690627e+04\n",
      "7  Meses desde el último evento+norm  2.529194e+03\n",
      "0                        ID Empleado  1.906651e+03\n",
      "3                Rol de Trabajo+norm  1.746140e+03\n",
      "6             Tamaño de Empresa+norm  1.060711e+03\n",
      "4               Ingreso Mensual+norm  9.291019e+02\n",
      "5              Distancia a Casa+norm  7.860848e+02\n",
      "9      Reputación de la Empresa+norm  2.527854e+02\n",
      "8                Trabajo Remoto+norm  1.571239e+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados para Satisfacción de Trabajo+norm:\n",
      "\n",
      "Modelo: RandomForestRegressor\n",
      "\n",
      "Resultados con todas las variables:\n",
      "R2: 1.0000\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "R2: 1.0000\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n",
      "\n",
      "Modelo: KNeighborsRegressor\n",
      "\n",
      "Resultados con todas las variables:\n",
      "R2: 0.0480\n",
      "RMSE: 0.2454\n",
      "MAE: 0.1968\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "R2: 0.0306\n",
      "RMSE: 0.2476\n",
      "MAE: 0.1989\n",
      "\n",
      "Modelo: LinearRegression\n",
      "\n",
      "Resultados con todas las variables:\n",
      "R2: 1.0000\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "R2: 1.0000\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n",
      "\n",
      "Resultados para Satisfacción de Trabajo+estand:\n",
      "\n",
      "Modelo: RandomForestRegressor\n",
      "\n",
      "Resultados con todas las variables:\n",
      "R2: 1.0000\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "R2: 1.0000\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n",
      "\n",
      "Modelo: KNeighborsRegressor\n",
      "\n",
      "Resultados con todas las variables:\n",
      "R2: -0.0302\n",
      "RMSE: 1.0047\n",
      "MAE: 0.8075\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "R2: -0.0219\n",
      "RMSE: 1.0007\n",
      "MAE: 0.8034\n",
      "\n",
      "Modelo: LinearRegression\n",
      "\n",
      "Resultados con todas las variables:\n",
      "R2: 1.0000\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "R2: 1.0000\n",
      "RMSE: 0.0000\n",
      "MAE: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "file_path = 'archivo_EN.csv'  # Cambia esto si el archivo está en otra ubicación\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrar las columnas que terminan en '+norm' y '+estand'\n",
    "norm_columns = [col for col in df.columns if col.endswith('+norm')]\n",
    "estand_columns = [col for col in df.columns if col.endswith('+estand')]\n",
    "\n",
    "# Dividir los datos en variables predictoras (X) y variables objetivo (y) para cada caso\n",
    "# Para la predicción de Satisfacción de Trabajo+norm\n",
    "X_norm = df.drop(columns=norm_columns)\n",
    "y_norm = df[norm_columns[0]]  # Asumimos que solo hay una columna de 'Satisfacción de Trabajo+norm'\n",
    "\n",
    "# Para la predicción de Satisfacción de Trabajo+estand\n",
    "X_estand = df.drop(columns=estand_columns)\n",
    "y_estand = df[estand_columns[0]]  # Asumimos que solo hay una columna de 'Satisfacción de Trabajo+estand'\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (80% - 20%) para cada caso\n",
    "X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(X_norm, y_norm, test_size=0.2, random_state=42)\n",
    "X_train_estand, X_test_estand, y_train_estand, y_test_estand = train_test_split(X_estand, y_estand, test_size=0.2, random_state=42)\n",
    "\n",
    "# Función para seleccionar las mejores características con SelectKBest\n",
    "def select_best_features(X_train, X_test, y_train, k=10):\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)  # Usamos f_regression para evaluar la relevancia\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    # Obtener las mejores características y sus puntuaciones\n",
    "    best_feature_indices = selector.get_support(indices=True)\n",
    "    best_features = X_train.columns[best_feature_indices]\n",
    "    feature_scores = selector.scores_[best_feature_indices]\n",
    "    \n",
    "    # Crear un DataFrame con las mejores características y sus puntuaciones\n",
    "    best_features_df = pd.DataFrame({\n",
    "        'Feature': best_features,\n",
    "        'Score': feature_scores\n",
    "    }).sort_values(by='Score', ascending=False)\n",
    "    \n",
    "    return X_train_selected, X_test_selected, best_features_df\n",
    "\n",
    "# Seleccionar las mejores características para ambos casos (norm y estand)\n",
    "X_train_norm_selected, X_test_norm_selected, best_features_norm_df = select_best_features(X_train_norm, X_test_norm, y_train_norm)\n",
    "X_train_estand_selected, X_test_estand_selected, best_features_estand_df = select_best_features(X_train_estand, X_test_estand, y_train_estand)\n",
    "\n",
    "# Mostrar las mejores características seleccionadas para cada caso\n",
    "print(\"Mejores características para Satisfacción de Trabajo+norm y sus puntuaciones:\")\n",
    "print(best_features_norm_df)\n",
    "\n",
    "print(\"\\nMejores características para Satisfacción de Trabajo+estand y sus puntuaciones:\")\n",
    "print(best_features_estand_df)\n",
    "\n",
    "# Modelos de regresión\n",
    "models = {\n",
    "    'RandomForestRegressor': RandomForestRegressor(random_state=42),\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'LinearRegression': LinearRegression()\n",
    "}\n",
    "\n",
    "# Función para evaluar los modelos\n",
    "def evaluate_models(models, X_train, X_test, y_train, y_test, X_train_selected, X_test_selected):\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        # Entrenamiento con todas las variables\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results[model_name] = {\n",
    "            'r2_all': r2_score(y_test, y_pred),  # R2 para medir la proporción de variación explicada\n",
    "            'rmse_all': mean_squared_error(y_test, y_pred, squared=False),  # RMSE para medir el error promedio\n",
    "            'mae_all': mean_absolute_error(y_test, y_pred)  # MAE para medir el error absoluto promedio\n",
    "        }\n",
    "\n",
    "        # Entrenamiento con selección de características\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred_selected = model.predict(X_test_selected)\n",
    "        results[model_name].update({\n",
    "            'r2_selected': r2_score(y_test, y_pred_selected),  # R2 para las variables seleccionadas\n",
    "            'rmse_selected': mean_squared_error(y_test, y_pred_selected, squared=False),\n",
    "            'mae_selected': mean_absolute_error(y_test, y_pred_selected)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluar los modelos para Satisfacción de Trabajo+norm\n",
    "results_norm = evaluate_models(models, X_train_norm, X_test_norm, y_train_norm, y_test_norm, X_train_norm_selected, X_test_norm_selected)\n",
    "\n",
    "# Evaluar los modelos para Satisfacción de Trabajo+estand\n",
    "results_estand = evaluate_models(models, X_train_estand, X_test_estand, y_train_estand, y_test_estand, X_train_estand_selected, X_test_estand_selected)\n",
    "\n",
    "# Mostrar resultados para Satisfacción de Trabajo+norm\n",
    "print(\"\\nResultados para Satisfacción de Trabajo+norm:\")\n",
    "for model_name, metrics in results_norm.items():\n",
    "    print(f\"\\nModelo: {model_name}\")\n",
    "    print(\"\\nResultados con todas las variables:\")\n",
    "    print(f\"R2: {metrics['r2_all']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rmse_all']:.4f}\")\n",
    "    print(f\"MAE: {metrics['mae_all']:.4f}\")\n",
    "    \n",
    "    print(\"\\nResultados con selección de las mejores variables:\")\n",
    "    print(f\"R2: {metrics['r2_selected']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rmse_selected']:.4f}\")\n",
    "    print(f\"MAE: {metrics['mae_selected']:.4f}\")\n",
    "\n",
    "# Mostrar resultados para Satisfacción de Trabajo+estand\n",
    "print(\"\\nResultados para Satisfacción de Trabajo+estand:\")\n",
    "for model_name, metrics in results_estand.items():\n",
    "    print(f\"\\nModelo: {model_name}\")\n",
    "    print(\"\\nResultados con todas las variables:\")\n",
    "    print(f\"R2: {metrics['r2_all']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rmse_all']:.4f}\")\n",
    "    print(f\"MAE: {metrics['mae_all']:.4f}\")\n",
    "    \n",
    "    print(\"\\nResultados con selección de las mejores variables:\")\n",
    "    print(f\"R2: {metrics['r2_selected']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rmse_selected']:.4f}\")\n",
    "    print(f\"MAE: {metrics['mae_selected']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento con Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores características para Satisfacción de Trabajo+norm y sus puntuaciones:\n",
      "                               Feature         Score\n",
      "1                                 Edad  1.413342e+19\n",
      "5                          Edad+estand  5.462192e+17\n",
      "6      Antigüedad en la Empresa+estand  1.690627e+04\n",
      "2             Antigüedad en la Empresa  1.690627e+04\n",
      "9  Meses desde el último evento+estand  2.529194e+03\n",
      "4         Meses desde el último evento  2.529194e+03\n",
      "0                          ID Empleado  1.906651e+03\n",
      "3                       Rol de Trabajo  1.746140e+03\n",
      "7                Rol de Trabajo+estand  1.746140e+03\n",
      "8             Tamaño de Empresa+estand  1.060711e+03\n",
      "\n",
      "Mejores características para Satisfacción de Trabajo+estand y sus puntuaciones:\n",
      "                             Feature         Score\n",
      "1                               Edad  1.130674e+20\n",
      "6                          Edad+norm  2.737709e+17\n",
      "7      Antigüedad en la Empresa+norm  1.690627e+04\n",
      "2           Antigüedad en la Empresa  1.690627e+04\n",
      "9  Meses desde el último evento+norm  2.529194e+03\n",
      "5       Meses desde el último evento  2.529194e+03\n",
      "0                        ID Empleado  1.906651e+03\n",
      "8                Rol de Trabajo+norm  1.746140e+03\n",
      "3                     Rol de Trabajo  1.746140e+03\n",
      "4                  Tamaño de Empresa  1.060711e+03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m results_norm \u001b[38;5;241m=\u001b[39m evaluate_models(models, X_train_norm, X_test_norm, y_train_norm, y_test_norm, X_train_norm_selected, X_test_norm_selected)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Evaluar los modelos para Satisfacción de Trabajo+estand\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m results_estand \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_estand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_estand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_estand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_estand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_estand_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_estand_selected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Mostrar resultados para Satisfacción de Trabajo+norm\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResultados para Satisfacción de Trabajo+norm:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 82\u001b[0m, in \u001b[0;36mevaluate_models\u001b[1;34m(models, X_train, X_test, y_train, y_test, X_train_selected, X_test_selected)\u001b[0m\n\u001b[0;32m     75\u001b[0m results[model_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2_all\u001b[39m\u001b[38;5;124m'\u001b[39m: r2_score(y_test, y_pred),  \u001b[38;5;66;03m# R² para regresión\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse_all\u001b[39m\u001b[38;5;124m'\u001b[39m: mean_squared_error(y_test, y_pred),  \u001b[38;5;66;03m# MSE para regresión\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae_all\u001b[39m\u001b[38;5;124m'\u001b[39m: mean_absolute_error(y_test, y_pred)  \u001b[38;5;66;03m# MAE para regresión\u001b[39;00m\n\u001b[0;32m     79\u001b[0m }\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Entrenamiento con selección de características\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m y_pred_selected \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_selected)\n\u001b[0;32m     84\u001b[0m results[model_name]\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2_selected\u001b[39m\u001b[38;5;124m'\u001b[39m: r2_score(y_test, y_pred_selected),  \u001b[38;5;66;03m# R² para las variables seleccionadas\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse_selected\u001b[39m\u001b[38;5;124m'\u001b[39m: mean_squared_error(y_test, y_pred_selected),\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae_selected\u001b[39m\u001b[38;5;124m'\u001b[39m: mean_absolute_error(y_test, y_pred_selected)\n\u001b[0;32m     88\u001b[0m })\n",
      "File \u001b[1;32mc:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\svm\\_base.py:328\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    314\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    318\u001b[0m (\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 328\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_weight_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "File \u001b[1;32m_libsvm.pyx:263\u001b[0m, in \u001b[0;36msklearn.svm._libsvm.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression  # Usamos LogisticRegression para Softmax\n",
    "from sklearn.feature_selection import SelectKBest, f_classif  # f_classif para clasificación\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "file_path = 'archivo_estandarizado_normalizado.csv'  # Cambia esto si el archivo está en otra ubicación\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrar las columnas que terminan en '+norm' y '+estand'\n",
    "norm_columns = [col for col in df.columns if col.endswith('+norm')]\n",
    "estand_columns = [col for col in df.columns if col.endswith('+estand')]\n",
    "\n",
    "# Dividir los datos en variables predictoras (X) y variables objetivo (y) para cada caso\n",
    "# Para la predicción de Satisfacción de Trabajo+norm\n",
    "X_norm = df.drop(columns=norm_columns)\n",
    "y_norm = df[norm_columns[0]]  # Asumimos que solo hay una columna de 'Satisfacción de Trabajo+norm'\n",
    "\n",
    "# Para la predicción de Satisfacción de Trabajo+estand\n",
    "X_estand = df.drop(columns=estand_columns)\n",
    "y_estand = df[estand_columns[0]]  # Asumimos que solo hay una columna de 'Satisfacción de Trabajo+estand'\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (80% - 20%) para cada caso\n",
    "X_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(X_norm, y_norm, test_size=0.2, random_state=42)\n",
    "X_train_estand, X_test_estand, y_train_estand, y_test_estand = train_test_split(X_estand, y_estand, test_size=0.2, random_state=42)\n",
    "\n",
    "# Función para seleccionar las mejores características con SelectKBest\n",
    "def select_best_features(X_train, X_test, y_train, k=10):\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)  # Usamos f_classif para clasificación\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    # Obtener las mejores características y sus puntuaciones\n",
    "    best_feature_indices = selector.get_support(indices=True)\n",
    "    best_features = X_train.columns[best_feature_indices]\n",
    "    feature_scores = selector.scores_[best_feature_indices]\n",
    "    \n",
    "    # Crear un DataFrame con las mejores características y sus puntuaciones\n",
    "    best_features_df = pd.DataFrame({\n",
    "        'Feature': best_features,\n",
    "        'Score': feature_scores\n",
    "    }).sort_values(by='Score', ascending=False)\n",
    "    \n",
    "    return X_train_selected, X_test_selected, best_features_df\n",
    "\n",
    "# Seleccionar las mejores características para ambos casos (norm y estand)\n",
    "X_train_norm_selected, X_test_norm_selected, best_features_norm_df = select_best_features(X_train_norm, X_test_norm, y_train_norm)\n",
    "X_train_estand_selected, X_test_estand_selected, best_features_estand_df = select_best_features(X_train_estand, X_test_estand, y_train_estand)\n",
    "\n",
    "# Mostrar las mejores características seleccionadas para cada caso\n",
    "print(\"Mejores características para Satisfacción de Trabajo+norm y sus puntuaciones:\")\n",
    "print(best_features_norm_df)\n",
    "\n",
    "print(\"\\nMejores características para Satisfacción de Trabajo+estand y sus puntuaciones:\")\n",
    "print(best_features_estand_df)\n",
    "\n",
    "# Modelos de clasificación (Softmax a través de LogisticRegression)\n",
    "models = {\n",
    "    'LogisticRegression (Softmax)': LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', random_state=42)\n",
    "}\n",
    "\n",
    "# Función para evaluar los modelos\n",
    "def evaluate_models(models, X_train, X_test, y_train, y_test, X_train_selected, X_test_selected):\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        # Entrenamiento con todas las variables\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results[model_name] = {\n",
    "            'accuracy_all': accuracy_score(y_test, y_pred),  # Accuracy para clasificación\n",
    "            'classification_report_all': classification_report(y_test, y_pred),  # Precision, Recall, F1-score\n",
    "            'confusion_matrix_all': confusion_matrix(y_test, y_pred)  # Matriz de confusión\n",
    "        }\n",
    "\n",
    "        # Entrenamiento con selección de características\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred_selected = model.predict(X_test_selected)\n",
    "        results[model_name].update({\n",
    "            'accuracy_selected': accuracy_score(y_test, y_pred_selected),  # Accuracy para las variables seleccionadas\n",
    "            'classification_report_selected': classification_report(y_test, y_pred_selected),\n",
    "            'confusion_matrix_selected': confusion_matrix(y_test, y_pred_selected)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluar los modelos para Satisfacción de Trabajo+norm\n",
    "results_norm = evaluate_models(models, X_train_norm, X_test_norm, y_train_norm, y_test_norm, X_train_norm_selected, X_test_norm_selected)\n",
    "\n",
    "# Evaluar los modelos para Satisfacción de Trabajo+estand\n",
    "results_estand = evaluate_models(models, X_train_estand, X_test_estand, y_train_estand, y_test_estand, X_train_estand_selected, X_test_estand_selected)\n",
    "\n",
    "# Mostrar resultados para Satisfacción de Trabajo+norm\n",
    "print(\"\\nResultados para Satisfacción de Trabajo+norm:\")\n",
    "for model_name, metrics in results_norm.items():\n",
    "    print(f\"\\nModelo: {model_name}\")\n",
    "    print(\"\\nResultados con todas las variables:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy_all']:.4f}\")\n",
    "    print(f\"Reporte de clasificación:\\n{metrics['classification_report_all']}\")\n",
    "    print(f\"Matriz de confusión:\\n{metrics['confusion_matrix_all']}\")\n",
    "    \n",
    "    print(\"\\nResultados con selección de las mejores variables:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy_selected']:.4f}\")\n",
    "    print(f\"Reporte de clasificación:\\n{metrics['classification_report_selected']}\")\n",
    "    print(f\"Matriz de confusión:\\n{metrics['confusion_matrix_selected']}\")\n",
    "\n",
    "# Mostrar resultados para Satisfacción de Trabajo+estand\n",
    "print(\"\\nResultados para Satisfacción de Trabajo+estand:\")\n",
    "for model_name, metrics in results_estand.items():\n",
    "    print(f\"\\nModelo: {model_name}\")\n",
    "    print(\"\\nResultados con todas las variables:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy_all']:.4f}\")\n",
    "    print(f\"Reporte de clasificación:\\n{metrics['classification_report_all']}\")\n",
    "    print(f\"Matriz de confusión:\\n{metrics['confusion_matrix_all']}\")\n",
    "    \n",
    "    print(\"\\nResultados con selección de las mejores variables:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy_selected']:.4f}\")\n",
    "    print(f\"Reporte de clasificación:\\n{metrics['classification_report_selected']}\")\n",
    "    print(f\"Matriz de confusión:\\n{metrics['confusion_matrix_selected']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de todos los Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores características para predecir Satisfacción del Trabajo y sus puntuaciones:\n",
      "                        Feature       Score\n",
      "1                Rol de Trabajo  536.723052\n",
      "3             Tamaño de Empresa  406.954854\n",
      "2              Distancia a Casa  311.023650\n",
      "7      Reputación de la Empresa  118.366184\n",
      "4  Meses desde el último evento   86.156069\n",
      "0      Antigüedad en la Empresa   76.708110\n",
      "5                Trabajo Remoto   58.207989\n",
      "6   Oportunidades de Innovación   49.239033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados para clasificación:\n",
      "\n",
      "Modelo: KNN_all\n",
      "\n",
      "Resultados con todas las variables:\n",
      "Accuracy: 0.6091\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.59      0.62      1693\n",
      "           1       0.50      0.47      0.49      2537\n",
      "           2       0.60      0.73      0.65      5222\n",
      "           3       0.72      0.54      0.62      3102\n",
      "\n",
      "    accuracy                           0.61     12554\n",
      "   macro avg       0.62      0.58      0.59     12554\n",
      "weighted avg       0.62      0.61      0.61     12554\n",
      "\n",
      "Matriz de confusión:\n",
      "[[1002  136  483   72]\n",
      " [ 130 1197 1036  174]\n",
      " [ 282  762 3788  390]\n",
      " [ 115  289 1038 1660]]\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "\n",
      "No se encontró la evaluación con las mejores variables seleccionadas.\n",
      "\n",
      "Modelo: RF_all\n",
      "\n",
      "Resultados con todas las variables:\n",
      "Accuracy: 0.6788\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.55      0.71      1693\n",
      "           1       0.97      0.36      0.52      2537\n",
      "           2       0.57      0.99      0.72      5222\n",
      "           3       0.96      0.49      0.65      3102\n",
      "\n",
      "    accuracy                           0.68     12554\n",
      "   macro avg       0.87      0.60      0.65     12554\n",
      "weighted avg       0.80      0.68      0.66     12554\n",
      "\n",
      "Matriz de confusión:\n",
      "[[ 936    0  745   12]\n",
      " [   3  902 1621   11]\n",
      " [   3   19 5165   35]\n",
      " [   0    8 1575 1519]]\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "\n",
      "No se encontró la evaluación con las mejores variables seleccionadas.\n",
      "\n",
      "Modelo: LR_all\n",
      "\n",
      "Resultados con todas las variables:\n",
      "Accuracy: 0.4199\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1693\n",
      "           1       0.00      0.00      0.00      2537\n",
      "           2       0.44      0.88      0.59      5222\n",
      "           3       0.32      0.22      0.26      3102\n",
      "\n",
      "    accuracy                           0.42     12554\n",
      "   macro avg       0.19      0.27      0.21     12554\n",
      "weighted avg       0.26      0.42      0.31     12554\n",
      "\n",
      "Matriz de confusión:\n",
      "[[   0    0 1299  394]\n",
      " [   0    0 2128  409]\n",
      " [   0    0 4600  622]\n",
      " [   0    0 2431  671]]\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "\n",
      "No se encontró la evaluación con las mejores variables seleccionadas.\n",
      "\n",
      "Modelo: KNN_selected\n",
      "\n",
      "Resultados con todas las variables:\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "Accuracy: 0.5190\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.41      0.45      1693\n",
      "           1       0.41      0.40      0.40      2537\n",
      "           2       0.55      0.68      0.61      5222\n",
      "           3       0.59      0.40      0.48      3102\n",
      "\n",
      "    accuracy                           0.52     12554\n",
      "   macro avg       0.51      0.47      0.48     12554\n",
      "weighted avg       0.52      0.52      0.51     12554\n",
      "\n",
      "Matriz de confusión:\n",
      "[[ 701  218  616  158]\n",
      " [ 163 1009 1149  216]\n",
      " [ 353  819 3572  478]\n",
      " [ 223  438 1208 1233]]\n",
      "\n",
      "Modelo: RF_selected\n",
      "\n",
      "Resultados con todas las variables:\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "Accuracy: 0.5608\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.39      0.49      1693\n",
      "           1       0.55      0.31      0.40      2537\n",
      "           2       0.54      0.82      0.65      5222\n",
      "           3       0.60      0.42      0.50      3102\n",
      "\n",
      "    accuracy                           0.56     12554\n",
      "   macro avg       0.59      0.49      0.51     12554\n",
      "weighted avg       0.57      0.56      0.54     12554\n",
      "\n",
      "Matriz de confusión:\n",
      "[[ 655  136  732  170]\n",
      " [  79  797 1432  229]\n",
      " [ 133  346 4278  465]\n",
      " [  96  178 1518 1310]]\n",
      "\n",
      "Modelo: LR_selected\n",
      "\n",
      "Resultados con todas las variables:\n",
      "\n",
      "Resultados con selección de las mejores variables:\n",
      "Accuracy: 0.4180\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1693\n",
      "           1       0.00      0.00      0.00      2537\n",
      "           2       0.43      0.94      0.59      5222\n",
      "           3       0.31      0.11      0.17      3102\n",
      "\n",
      "    accuracy                           0.42     12554\n",
      "   macro avg       0.19      0.26      0.19     12554\n",
      "weighted avg       0.26      0.42      0.29     12554\n",
      "\n",
      "Matriz de confusión:\n",
      "[[   0    0 1458  235]\n",
      " [   0    0 2327  210]\n",
      " [   0    0 4893  329]\n",
      " [   0    0 2748  354]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\General\\Cuarto\\2°do Cuatrimestre\\Machine Learning\\EDA\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif  # f_classif para clasificación\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest\n",
    "from sklearn.linear_model import LogisticRegression  # Algoritmo adicional: Regresión Logística\n",
    "from sklearn.preprocessing import LabelEncoder  # Para la codificación de etiquetas\n",
    "\n",
    "# Cargar el conjunto de datos\n",
    "file_path = 'archivo_int64.csv'  # Cambia esto si el archivo está en otra ubicación\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrar las columnas que terminan en '+norm' y '+estand'\n",
    "norm_columns = [col for col in df.columns if col.endswith('+norm')]\n",
    "estand_columns = [col for col in df.columns if col.endswith('+estand')]\n",
    "\n",
    "# Eliminar las columnas '+norm', '+estand' y 'ID Empleado' para obtener solo las variables relevantes\n",
    "columns_to_drop = norm_columns + estand_columns + ['ID Empleado']\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Asegurarse de que la columna 'Satisfacción del Trabajo' exista\n",
    "target_column = 'Satisfacción del Trabajo'\n",
    "if target_column not in df_cleaned.columns:\n",
    "    raise ValueError(f\"La columna '{target_column}' no se encuentra en el DataFrame\")\n",
    "\n",
    "# Dividir los datos en variables predictoras (X) y la variable objetivo 'Satisfacción del Trabajo'\n",
    "X = df_cleaned.drop(columns=[target_column])  # Todas las columnas excepto 'Satisfacción del Trabajo'\n",
    "y = df_cleaned[target_column]  # 'Satisfacción del Trabajo' como la variable objetivo\n",
    "\n",
    "# Convertir las etiquetas de 'Satisfacción del Trabajo' en valores numéricos enteros para clasificación\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (80% - 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Variable para controlar el número de características a seleccionar\n",
    "k_best_features = 8  # Modifica este valor según la cantidad de características que quieras usar\n",
    "\n",
    "# Función para seleccionar las mejores características con SelectKBest\n",
    "def select_best_features(X_train, X_test, y_train, k=10):\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)  # Usamos f_classif para clasificación\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    # Obtener las mejores características y sus puntuaciones\n",
    "    best_feature_indices = selector.get_support(indices=True)\n",
    "    best_features = X_train.columns[best_feature_indices]\n",
    "    feature_scores = selector.scores_[best_feature_indices]\n",
    "    \n",
    "    # Crear un DataFrame con las mejores características y sus puntuaciones\n",
    "    best_features_df = pd.DataFrame({\n",
    "        'Feature': best_features,\n",
    "        'Score': feature_scores\n",
    "    }).sort_values(by='Score', ascending=False)\n",
    "    \n",
    "    return X_train_selected, X_test_selected, best_features_df\n",
    "\n",
    "# Seleccionar las mejores características\n",
    "X_train_selected, X_test_selected, best_features_df = select_best_features(X_train, X_test, y_train, k=k_best_features)\n",
    "\n",
    "# Mostrar las mejores características seleccionadas\n",
    "print(\"Mejores características para predecir Satisfacción del Trabajo y sus puntuaciones:\")\n",
    "print(best_features_df)\n",
    "\n",
    "# Función para evaluar los modelos\n",
    "def evaluate_models(X_train, X_test, y_train, y_test, X_train_selected, X_test_selected):\n",
    "    results = {}\n",
    "    \n",
    "    # Inicializar los modelos (KNN, Random Forest y Regresión Logística)\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=5)  # Modelo KNN\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # Modelo Random Forest\n",
    "    lr_model = LogisticRegression(random_state=42, max_iter=1000)  # Aumentar max_iter para evitar advertencias\n",
    "    \n",
    "    # Evaluar el modelo KNN con todas las características\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_knn = knn_model.predict(X_test)\n",
    "    results['KNN_all'] = {\n",
    "        'accuracy_all': accuracy_score(y_test, y_pred_knn),\n",
    "        'classification_report_all': classification_report(y_test, y_pred_knn),\n",
    "        'confusion_matrix_all': confusion_matrix(y_test, y_pred_knn)\n",
    "    }\n",
    "\n",
    "    # Evaluar el modelo Random Forest con todas las características\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    results['RF_all'] = {\n",
    "        'accuracy_all': accuracy_score(y_test, y_pred_rf),\n",
    "        'classification_report_all': classification_report(y_test, y_pred_rf),\n",
    "        'confusion_matrix_all': confusion_matrix(y_test, y_pred_rf)\n",
    "    }\n",
    "\n",
    "    # Evaluar el modelo Regresión Logística con todas las características\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    y_pred_lr = lr_model.predict(X_test)\n",
    "    results['LR_all'] = {\n",
    "        'accuracy_all': accuracy_score(y_test, y_pred_lr),\n",
    "        'classification_report_all': classification_report(y_test, y_pred_lr),\n",
    "        'confusion_matrix_all': confusion_matrix(y_test, y_pred_lr)\n",
    "    }\n",
    "\n",
    "    # Evaluación con selección de características para KNN\n",
    "    knn_model.fit(X_train_selected, y_train)\n",
    "    y_pred_knn_selected = knn_model.predict(X_test_selected)\n",
    "    results['KNN_selected'] = {\n",
    "        'accuracy_selected': accuracy_score(y_test, y_pred_knn_selected),\n",
    "        'classification_report_selected': classification_report(y_test, y_pred_knn_selected),\n",
    "        'confusion_matrix_selected': confusion_matrix(y_test, y_pred_knn_selected)\n",
    "    }\n",
    "\n",
    "    # Evaluación con selección de características para Random Forest\n",
    "    rf_model.fit(X_train_selected, y_train)\n",
    "    y_pred_rf_selected = rf_model.predict(X_test_selected)\n",
    "    results['RF_selected'] = {\n",
    "        'accuracy_selected': accuracy_score(y_test, y_pred_rf_selected),\n",
    "        'classification_report_selected': classification_report(y_test, y_pred_rf_selected),\n",
    "        'confusion_matrix_selected': confusion_matrix(y_test, y_pred_rf_selected)\n",
    "    }\n",
    "\n",
    "    # Evaluación con selección de características para Regresión Logística\n",
    "    lr_model.fit(X_train_selected, y_train)\n",
    "    y_pred_lr_selected = lr_model.predict(X_test_selected)\n",
    "    results['LR_selected'] = {\n",
    "        'accuracy_selected': accuracy_score(y_test, y_pred_lr_selected),\n",
    "        'classification_report_selected': classification_report(y_test, y_pred_lr_selected),\n",
    "        'confusion_matrix_selected': confusion_matrix(y_test, y_pred_lr_selected)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluar los resultados\n",
    "results = evaluate_models(X_train, X_test, y_train, y_test, X_train_selected, X_test_selected)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\nResultados para clasificación:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\nModelo: {model_name}\")\n",
    "    print(f\"\\nResultados con todas las variables:\")\n",
    "    if 'accuracy_all' in metrics:\n",
    "        print(f\"Accuracy: {metrics['accuracy_all']:.4f}\")\n",
    "        print(f\"Reporte de clasificación:\\n{metrics['classification_report_all']}\")\n",
    "        print(f\"Matriz de confusión:\\n{metrics['confusion_matrix_all']}\")\n",
    "    \n",
    "    print(\"\\nResultados con selección de las mejores variables:\")\n",
    "    if 'accuracy_selected' in metrics:\n",
    "        print(f\"Accuracy: {metrics['accuracy_selected']:.4f}\")\n",
    "        print(f\"Reporte de clasificación:\\n{metrics['classification_report_selected']}\")\n",
    "        print(f\"Matriz de confusión:\\n{metrics['confusion_matrix_selected']}\")\n",
    "    else:\n",
    "        print(\"\\nNo se encontró la evaluación con las mejores variables seleccionadas.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
